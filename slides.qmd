---
title: "Using Vector Embeddings For Sentiment Analysis"
author: "Rod Acosta, Kevin Furbish, Ibrahim Khan, Anthony Washington"
format: 
  revealjs:
    theme: moon
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

# Introduction {.smaller .scrollable}

- Sentiment Analysis:
  - Defined as a branch of Natural Language Processing (NLP) that focuses on the computational treatment of opinions, sentiments, and subjectivity in digital text [@MEDHAT20141093].

## Sentiment Analysis {.smaller .scrollable}

- Importance of Sentiment Analysis:
  - Essential for understanding public opinion, aiding businesses in refining marketing strategies, improving products, and enhancing customer satisfaction.
  - The rise of social media and e-commerce platforms has increased the importance of sentiment analysis for real-time consumer behavior insights.

- Case Study: Twitter Data:
  - Hasan, Maliha, and Arifuzzaman (2019) used Twitter data to demonstrate the potential of sentiment analysis.
  - Their methodology combined Bag of Words (BoW) and Term Frequency-Inverse Document Frequency (TF-IDF) models with logistic regression, achieving an accuracy of 85.25% in classifying tweets as positive or negative [@9036670].

## Sentiment Analysis (Continued) {.smaller}

::: {.panel-tabset .smaller}

### Case Study: E-commerce Reviews

- Kathuria, Sethi, and Negi (2022) applied sentiment analysis to e-commerce reviews using various machine learning models (logistic regression, AdaBoost, SVM, Naive Bayes, and random forest).
- Analyzed the Women’s E-commerce Clothing Reviews dataset to understand consumer behavior and the impact of electronic word-of-mouth (eWOM) on customer attitudes and product sales [@10093674].


### Case Study: Granular Approach

- Nasukawa and Yi (2003) focused on extracting sentiments linked to specific subjects rather than entire documents.
- Their prototype system used a syntactic parser and sentiment lexicon to detect sentiments in web pages and news articles, offering detailed insights into specific opinions [@nasukawa2003sentiment].

:::

---

- Conclusion:
  - These studies illustrate the evolution and application of sentiment analysis using NLP, highlighting its critical role in extracting insights from vast amounts of text data for strategic decision-making.
  
---

## Vector Embeddings In Natural Language Processing {.smaller}

- Introduction to Embeddings:
  - Vector embeddings are crucial for encoding or describing the sentiment of words or groups of words.
  - Embeddings improve upon representing words as indices in a vocabulary by encoding relationships or similarities between words [@camacho2020embeddings].

- Advantages of Embeddings:
  - Unlike simple vocabulary indices, embeddings can represent multiple meanings of words and their semantic and syntactic patterns [@pennington2014glove].
  - Example: In the Word2Vec model, vector arithmetic can evaluate analogies (e.g., "king" - "man" + "woman" = "queen")[@mikolov2013word2vec].

---

- Advanced Embedding Models:
  - Sentence embeddings build on word embeddings to represent sentences, improving over Bag of Words models, which have high dimensionality and sparseness issues[@pilehvar2020embeddings].
  - Document embeddings further extend this concept to encode entire documents for NLP tasks where word ordering is important[@pilehvar2020embeddings].

---

## Using Embeddings For Sentiment Analysis {.smaller}

- Performance Improvement:
  - Word embedding and deep learning models have significantly enhanced NLP tasks, including sentiment analysis [@10255396].
  - Popular methods include Word2Vec and Global Vectors (GloVe), trained on different datasets (Google News and Wikipedia 2014, respectively) [@10543279].

- Challenges with Pre-trained Embeddings:
  - Pre-trained word embeddings often fail to capture contextual sentiment information, leading to inaccuracies (e.g., mapping "good" and "bad" to neighboring vectors) [@9397340][@7296633].
  - The paper aims to explore these challenges and investigate alternative embedding methods for more accurate sentiment analysis.


# Methods

## What is a neural Network?
- A neural network is a type of algorithm that mimics the structure and function 
of the human brain. Their goal is to create an artificial system that can process
and analyze data in a similar way.
- There are different types of neural networks but there are some common elements
between most of them. Those elements are:
  - Artificial Neurons
  - Layers

## Neural Network Layers {.smaller}
- Neural networks usually have three types of layers:
  - Input Layer
  - Hidden layers
  - Output layer
  
![](nnLayers.png){fig-align="center"}

## What are embeddings? {.smaller}
- Embeddings are a technique that allow us to map words or phrases into a corresponding
vector of real numbers, where the position and direction of the vector capture the word's
semantic meaning in relation to other words. 
- They make high-dimensional data like words readable
to our algorithm/model and allows our model to recognize and learn meaningful relationships 
and similarities between words

:::: {.columns}
::: {.column width="40%"}
![](embLayer.png){fig-align="center"}
:::
::: {.column width="60%"}
![](How-Embeddings-Work.jpg){fig-align="center"}
:::
::::

## Dense Layer & Cosine Similarity {.smaller}
:::: {.columns}
::: {.column width="50%"}
- Cosine Similarity
  - Measures the cosine of the angle between two non-zero vectors, providing a measure of similarity.
  - The smaller the angle the higher the similarity between the two vectors.
  - $cosine\_similarity(u,v) = \frac{u.v}{||u|| ||v||}$
![](cosine.jpg){fig-align="center"}
:::
::: {.column width="50%"}
- Dense Layer
  - A logistic regression model with a sigmoid activation function used for binary classification. 
  - It outputs the probability that the input belongs to a positive class.
  - $y=\sigma(W⋅z+b)$
  - Where:
     - z is the flattened input vector.
     - W is the weight vector.
     - b is the bias term.
     - $\sigma(x) = \frac{1}{1+e^{-x}}$ is the sigmoid function.
:::
::::

## Sentiment Analysis
- Through the use of a neural network and it's hidden layers (embedding & dense), and the cosine similarity we are able to take inputs
and classify them as being part of a positive or negative class based on what our model has learned from our training dataset.

# Analysis and Results
## Dataset Description 

::: {.nonincremental}

- Data set of 25,000 movie reviews from IMDB
- Max 30 reviews for each movie since popular movies are rated more often than unpopular movies.
- Includes only the top 5,000 most frequent words, minus the top 50 most frequent words
- IMDB reviews 1-10 star rating converted to a 0-1 scale
- Reviews already in vectorized format

:::

## Vectorization

::: {.nonincremental}

- Neural Networks require numeric inputs, not the natural language of reviews
- Vectorization represents each word with a unique numeric substitution. For example the following texts: 
  - "this is fun", "fun times ahead", "fun is ahead of times"

- Results in a vocabulary of [this, is, fun, times, ahead, of]
- Vectorized observations: [1, 2, 3], [3, 4, 5] and [3, 2, 5, 6, 4]
- TensorFlow includes the IMDB dataset in a vectorized format

:::

# Statistical Modeling
## Neural Network Implementation {.smaller}

::: {.nonincremental}

- Neural network model implemented and trained using TF's Keras
- TF and Keras implemented in Python and accessed from R via Reticulate
- Model learns embeddings for each word in the vocabulary
- Multi-dimensional embedding vectors placed close in the learned vector-space to vectors of similar words
- Similar means having a similar contextual meaning in the training dataset and its sentiment classification
  - "gem" and "favorite" would be similar in context of a movie review, but not in a general context

:::

## Neural Network Implementation {.smaller}

::: {.nonincremental}

- Keras Sequential model (model is built layer-by-layer)
- The neural networks leverages the Keras embedding layer
  - Converts input vocabulary index into a vector of a chosen dimension
  - Dimensionality important hyperparameter that controls compression vs overfitting
- Output of embedding layer is the dimensionality hyperparameter
- Embedding layer connects to a dense layer
- Dense layers require a 1D input vector so embedding layer output is flattened to 1D
- Dense layer has a 1 output unit with sigmoid activation function that receives the flattened embedding output
- Output unit is the sentiment score
- Back-propagation trains embedding layer weights to be similar based on sentiment

:::

## Determining Vector Dimensionality

::: {.nonincremental}

- Number of dimensions for the embeddings must be selected
- Dimensionality selection often done ad hoc, or with grid search
- 2 through 7 dimensions were tested and compared by testing accuracy and qualitatively

:::

## Model Performance: 2D

::: {.nonincremental}

- 2 dimensions had best accuracy, but failed to capture meaning
- Closest embeddings to the embedding for "awful":

| word | cosine similarity |
| -----|-------------------|
| awful| 1.0 |
| lame | 1.0 |
| alcoholic | 1.0 |
| sadly | 0.99 |
| relevant | 0.99|
| are | 0.99 |

:::

## Model Performance: 2D vs 7D {.smaller}

::: {.nonincremental}

- 7 Dimensions had similar accuracy to 2, but better embedding performance

:::

::: {#tbl-panel layout-ncol=2}
| reference word | closest words |
| - | --- |
|awful | lame, alcoholic, sadly, relevant, are |
|mediocre | effort, turkey, terrible, stereotype, repeat |
|perfect | lovers, sing, manager, bath, donald | 
|favorite | deeply, roud, marie, polanski, poetry |

: 2 Dimensions {#tbl-first}

| reference word | closest words |
| - | --- |
|awful | ultimately, painful, sorry, fake, nowhere |
|mediocre | teeth, incompetent, main, disappointing, generous |
|perfect | great, seeking, freedom, tremendous, excellent |
|favorite | paulie, excellent, necessary, great, seeking |

: 7 Dimensions {#tbl-second}

Closest Words By Dimensionality
:::

```{r, results="hide"}
#| output: false
#| warning: false

#install.packages('tidyverse', repos = "http://cran.us.r-project.org")
#install.packages('tm', repos = "http://cran.us.r-project.org")
#install.packages('SnowballC', repos = "http://cran.us.r-project.org")
#install.packages('fastDummies', repos = "http://cran.us.r-project.org")
#install.packages('reticulate', repos = "http://cran.us.r-project.org")
#install.packages('tensorflow', repos = "http://cran.us.r-project.org")
#remove.packages("keras")
#install.packages('keras3', repos = "http://cran.us.r-project.org")
#install.packages('dplyr', repos = "http://cran.us.r-project.org")
#install.packages('text2vec', repos = "http://cran.us.r-project.org")
#install.packages('tidyr', repos = "http://cran.us.r-project.org")
#install.packages("devtools", repos = "http://cran.us.r-project.org")
#reticulate::install_python()
#install_keras(envname="r-tensorflow")

library(tidyverse)
library(tm)
library(SnowballC)
library(fastDummies)

library(text2vec)
library(dplyr)
library(tidyr)

library(keras3)
```
```{r}
max_vocab_words <- 5000
max_words <- 500

imdb <- dataset_imdb(num_words = max_vocab_words)
imdb_train_x = imdb$train$x
imdb_test_x = imdb$test$x
imdb_train_y = imdb$train$y
imdb_test_y = imdb$test$y

imdb_train_x <- pad_sequences(imdb_train_x, maxlen = max_words)
imdb_test_x <- pad_sequences(imdb_test_x, maxlen = max_words)
```

```{r,  results="hide"}
set.seed(3)
reticulate::py_run_string("import random; random.seed(3)")
reticulate::py_run_string("import keras; keras.utils.set_random_seed(3)")

max_vector_dimensions <- 7
accuracy_by_dimensions <- c()
for (num_dims in rep(2:max_vector_dimensions)) {
  imdb_embedding_model <- keras_model_sequential() %>%
    layer_embedding(input_dim = max_vocab_words, output_dim = num_dims) %>%
    layer_flatten() %>%
    layer_dense(units = 1, activation = "sigmoid")
  
  imdb_embedding_model %>% compile(
    optimizer = "rmsprop",
    loss = "binary_crossentropy",
    metrics = c("acc")
  )
  
  imdb_embedding_history <- imdb_embedding_model %>% keras3::fit(
    imdb_train_x, imdb_train_y, verbose=0,
    epochs = 6,
    batch_size = 32,
    validation_split = 0.2
  )
  
  imdb_embedding_results <- imdb_embedding_model %>% evaluate(imdb_test_x, imdb_test_y)
  accuracy_by_dimensions = c(accuracy_by_dimensions, imdb_embedding_results[["acc"]])
}
names(accuracy_by_dimensions) <- rep(2:max_vector_dimensions)
accuracy_by_dimensions
```

```{r, results="hide"}
best_num_dimensions = 2

imdb_embedding_model <- keras_model_sequential() %>%
  layer_embedding(input_dim = max_vocab_words, output_dim = best_num_dimensions) %>%
  layer_flatten() %>%
  layer_dense(units = 1, activation = "sigmoid")

imdb_embedding_model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("acc")
)

imdb_embedding_history <- imdb_embedding_model %>% keras3::fit(
  imdb_train_x, imdb_train_y, verbose=0,
  epochs = 6,
  batch_size = 32,
  validation_split = 0.2
)
  
imdb_embedding_results <- imdb_embedding_model %>% evaluate(imdb_test_x, imdb_test_y)

# Get the weights from layer 1, the embedding layer. This is
# the list of embedding vectors.
embedding_weights <- get_weights(imdb_embedding_model)[[1]] 

# add labels for the word represented by each embedding
imdb_word_index = dataset_imdb_word_index()
reverse_word_index <- names(imdb_word_index)
names(reverse_word_index) <- imdb_word_index
top_words <- reverse_word_index[as.character(1:max_vocab_words)]
rownames(embedding_weights) = c("*","*","*","*", top_words[1:(max_vocab_words-4)])

# create a function to find embeddings that are close to the desired 
# word, using cosine similarity to determine closeness
close_embeddings <- function(search_word, embeddings) {
  result <- embeddings[search_word, , drop=FALSE] %>% sim2(embeddings, y=., method="cosine")
  result[,1] %>% sort(decreasing=TRUE) %>% head(n=6)
}
```

```{r, results="hide"}
close_embeddings("awful", embedding_weights)
close_embeddings("mediocre", embedding_weights)
close_embeddings("perfect", embedding_weights)
close_embeddings("favorite", embedding_weights)
```

```{r, results="hide"}
best_num_dimensions = 7

imdb_embedding_model <- keras_model_sequential() %>%
  layer_embedding(input_dim = max_vocab_words, output_dim = best_num_dimensions) %>%
  layer_flatten() %>%
  layer_dense(units = 1, activation = "sigmoid")

imdb_embedding_model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("acc")
)

imdb_embedding_history <- imdb_embedding_model %>% keras3::fit(
  imdb_train_x, imdb_train_y, verbose=0,
  epochs = 6,
  batch_size = 32,
  validation_split = 0.2
)
  
imdb_embedding_results <- imdb_embedding_model %>% evaluate(imdb_test_x, imdb_test_y)
```

```{r, results="hide"}
embedding_weights <- get_weights(imdb_embedding_model)[[1]] 
rownames(embedding_weights) = c("*","*","*","*", top_words[1:(max_vocab_words-4)])

close_embeddings("awful", embedding_weights)
close_embeddings("mediocre", embedding_weights)
close_embeddings("perfect", embedding_weights)
close_embeddings("favorite", embedding_weights)
```

```{r,  results="hide"}
# Now that we know how many dimensions gives the greatest accuracy, find the number of epochs so model doesn't overfit
set.seed(42)
reticulate::py_run_string("import random; random.seed(42)")
reticulate::py_run_string("import keras; keras.utils.set_random_seed(42)")

imdb_embedding_model <- keras_model_sequential() %>%
  layer_embedding(input_dim = max_vocab_words, output_dim = best_num_dimensions) %>%
  layer_flatten() %>%
  layer_dense(units = 1, activation = "sigmoid")

imdb_embedding_model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("acc")
)

imdb_embedding_history <- imdb_embedding_model %>% keras3::fit(
  imdb_train_x, imdb_train_y, verbose=0,
  epochs = 10,
  batch_size = 32,
  validation_split = 0.2
)
  
imdb_embedding_results <- imdb_embedding_model %>% evaluate(imdb_test_x, imdb_test_y)
```


```{r,  results="hide", Umap_dataframe}
#install.packages("umap")
library(umap)
df_embedding_weights <- as.data.frame(embedding_weights, stringsAsFactors = FALSE)

df_embedding_weights <- cbind('words' = rownames(df_embedding_weights),df_embedding_weights)
rownames(df_embedding_weights) <- 1:nrow(df_embedding_weights)
```

## Data Visualization
- <font size="5"> The Umap below shows the location of where all the words are embedded on a 2D plane. The words of the IMDB dataset are clustered densely around the origin of the graph.</font>
```{r Umap}
library(ggplot2)
ggplot(df_embedding_weights) +
      geom_point(aes(x = V1, y = V2), colour = 'blue', size = 0.05)+
      labs(title = "Words embedding in 2D using UMAP")+
      theme(plot.title = element_text(hjust = .5, size = 14))
```

## Data Visualization
- <font size="5"> The partial Umap below zooms in to the center of the Umap where it is most dense and labels the words to display all the words that are located near each other based on the embedding. </font>
```{r Partial-Umap}

ggplot(df_embedding_weights[df_embedding_weights$V1 > -.005 & df_embedding_weights$V1 < .005 & df_embedding_weights$V2 > -.5,]) +
      geom_point(aes(x = V1, y = V2), colour = 'blue', size = 2) +
      geom_text(aes(V1, V2, label = words), size = 2.5, vjust=-1, hjust=0) +
      labs(title = "embedding in 2D using UMAP - partial view") +
      theme(plot.title = element_text(hjust = .5, size = 14))
```

## Data Visualization
- <font size="5">The Umap below shows the 25 most similar words to the word "angry" according to the embedding.</font>
```{r Umap-angry}
# Step 1: Select the embedding for the word "angry"
words <- embedding_weights["angry", , drop = FALSE]

# Step 2: Compute cosine similarity
cos_sim <- sim2(x = embedding_weights, y = words, method = "cosine", norm = "l2")

# Step 3: Select top 25 most similar words
select <- data.frame(words = rownames(as.data.frame(head(sort(cos_sim[, 1], decreasing = TRUE), 30))))

# Step 4: Join data frames
selected_words <- df_embedding_weights %>% inner_join(y = select, by = "words")


ggplot(selected_words, aes(x = V1, y = V2, colour = words == 'angry')) + 
      geom_point(show.legend = FALSE) + 
      scale_color_manual(values = c('black', 'red')) +
      geom_text(aes(V1, V2, label = words), show.legend = FALSE, size = 2.5, vjust=-1.5, hjust=0) +
      labs(title = "embedding of words related to 'angry'") +
      theme(plot.title = element_text(hjust = .5, size = 14))
```

## Conclusion
- The development and analysis of the word embedding model for classifying IMDB movie reviews demonstrated promising results. 

- The optimal number of embedding dimensions was identified as 7, achieving an accuracy of 87.34% on the test data set. 

- This was determined through extensive experimentation, revealing that higher dimensions, such as 7, provided competitive and consistent accuracy. 

## Conclusion
- The model's performance is noteworthy, given the constraints of training on only the top 5000 most common words, minimal data preprocessing, and limiting input sequences to the first 500 words of each review. 

- These factors illustrate the model's robustness and effectiveness in capturing the semantic relationships within the data.

## Conclusion
- <font size="6">The embedding similarity results showed that the model could meaningfully capture semantic relationships, as evidenced by the coherent and relevant similar words found for terms like "awful," "mediocre," "perfect," and "favorite." The final training session, capped at 10 epochs, ensured the model did not overfit, maintaining its accuracy and reliability.</font> 

- <font size="6"> Overall, the model's strong performance under constrained conditions highlights its potential for practical applications in sentiment analysis, offering an efficient and effective solution for understanding and categorizing movie reviews.</font>

## References