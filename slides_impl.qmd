---
title: "Using Vector Embeddings For Sentiment Analysis"
author: "Rod Acosta, Kevin Furbish, Ibrahim Khan, Anthony Washington"
format: revealjs
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

## Analysis and Results
### Dataset Description
- Data set of 25,000 movie reviews from IMDB
- Max 30 reviews for each movie since popular movies are rated more often than unpopular movies.
- Includes only the top 5,000 most frequent words, minus the top 50 most frequent words
- IMDB reviews 1-10 star rating converted to a 0-1 scale
- Reviews already in vectorized format

## Vectorization
- Neural Networks require numeric inputs, not the natural language of reviews
- Vectorization represents each word with a unique numeric substitution. For example the following texts: 
  - "this is fun", "fun times ahead", "fun is ahead of times"

- Results in a vocabulary of [this, is, fun, times, ahead, of]
- Vectorized observations: [1, 2, 3], [3, 4, 5] and [3, 2, 5, 6, 4]
- TensorFlow includes the IMDB dataset in a vectorized format

## Statistical Modeling {.smaller}

- Neural network model implemented and trained using TF's Keras
- TF and Keras implemented in Python and accessed from R via Reticulate
- Model learns embeddings for each word in the vocabulary
- Multi-dimensional embedding vectors placed close in the learned vector-space to vectors of similar words
- Similar means having a similar contextual meaning in the training dataset and its sentiment classification
  - "gem" and "favorite" would be similar in context of a movie review, but not in a general context


## Neural Network Implementation {.smaller}
- Keras Sequential model (model is built layer-by-layer)
- The neural networks leverages the Keras embedding layer
  - Converts input vocabulary index into a vector of a chosen dimension
  - Dimensionality important hyperparameter that controls compression vs overfitting
- Output of embedding layer is the dimensionality hyperparameter
- Embedding layer connects to a dense layer
- Dense layers require a 1D input vector so embedding layer output is flattened to 1D
- Dense layer has a 1 output unit with sigmoid activation function that receives the flattened embedding output
- Output unit is the sentiment score
- Back-propagation trains embedding layer weights to be similar based on sentiment

## Determining Vector Dimensionality
- Number of dimensions for the embeddings must be selected
- Dimensionality selection often done ad hoc, or with grid search
- 2 through 7 dimensions were tested and compared by testing accuracy and qualitatively

## Model Performance: 2D
- 2 dimensions had best accuracy, but failed to capture meaning
- Closest embeddings to the embedding for "awful":

| word | cosine similarity |
| -----|-------------------|
| awful| 1.0 |
| lame | 1.0 |
| alcoholic | 1.0 |
| sadly | 0.99 |
| relevant | 0.99|
| are | 0.99 |

## Model Performance: 2D vs 7D {.smaller}
- Dimensions had similar accuracy, but better embedding performance

::: {#tbl-panel layout-ncol=2}
| reference word | closest words |
| - | --- |
|awful | lame, alcoholic, sadly, relevant, are |
|mediocre | effort, turkey, terrible, stereotype, repeat |
|perfect | lovers, sing, manager, bath, donald | 
|favorite | deeply, roud, marie, polanski, poetry |

: 2 Dimensions {#tbl-first}

| reference word | closest words |
| - | --- |
|awful | ultimately, painful, sorry, fake, nowhere |
|mediocre | teeth, incompetent, main, disappointing, generous |
|perfect | great, seeking, freedom, tremendous, excellent |
|favorite | paulie, excellent, necessary, great, seeking |

: 7 Dimensions {#tbl-second}

Closest Words By Dimensionality
:::


