---
title: "Using Vector Embeddings For Sentiment Analysis"
author: "Rod Acosta, Kevin Furbish, Ibrahim Khan, Anthony Washington"
format:
  html:
    code-fold: true
    toc: true
    toc-depth: 2
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

## 1. Introduction

Sentiment Analysis (SA) is a branch of Natural Language Processing (NLP) that computationally extracts the “treatment of opinions, sentiments and subjectivity” [@MEDHAT20141093] of digital text. 

### 1.1 Sentiment Analysis
Sentiment analysis, a crucial facet of natural language processing (NLP), involves interpreting and classifying emotions within text data. By understanding public opinion, businesses can refine marketing strategies, improve products, and enhance customer satisfaction. The rapid growth of social media and e-commerce platforms has amplified the importance of sentiment analysis, enabling real-time feedback and insights into consumer behavior.

The study by Hasan, Maliha, and Arifuzzaman (2019) demonstrates the potential of Twitter data for sentiment analysis, highlighting the application of NLP frameworks to gauge public opinion on products. Their methodology integrates the Bag of Words (BoW) and Term Frequency-Inverse Document Frequency (TF-IDF) models, coupled with logistic regression, to classify tweets as positive or negative with an accuracy of 85.25%. This approach underscores the benefits of combining BoW and TF-IDF to enhance sentiment analysis precision. [@9036670]

In a similar vein, Kathuria, Sethi, and Negi (2022) explored sentiment analysis on e-commerce reviews, employing machine learning (ML) models such as logistic regression, AdaBoost, SVM, Naive Bayes, and random forest. Utilizing the Women’s E-commerce Clothing Reviews dataset, they analyzed review texts, ratings, and recommendations to understand consumer behavior. Their findings underscore the significance of electronic word-of-mouth (eWOM) in shaping customer attitudes and product sales, providing e-commerce businesses with actionable insights to improve marketing strategies and customer satisfaction. [@10093674]

The foundational work by Nasukawa and Yi (2003) emphasizes a more granular approach to sentiment analysis, focusing on extracting sentiments linked to specific subjects rather than entire documents. Their prototype system, employing a syntactic parser and a sentiment lexicon, achieves high precision in detecting sentiments in web pages and news articles. By concentrating on local sentiment expressions, their method offers detailed insights into specific opinions, aiding businesses in monitoring public opinion and addressing unfavorable sentiments effectively. [@nasukawa2003sentiment]

Collectively, these studies highlight the evolution and application of sentiment analysis using NLP, illustrating its critical role in extracting valuable insights from vast amounts of text data. By leveraging advanced NLP techniques and ML models, businesses can gain a deeper understanding of consumer sentiment, thereby enhancing their strategic decision-making processes.

### 1.2 Vector Embeddings In Natural Language Processing
One technique for encoding or describing the sentiment of a word or group of words is to use vector embeddings (or embeddings). Before discussing the use of embeddings for SA, it’s important to first understand what embeddings are. Embeddings are said to be “one of the most important topics of interest” in NLP in the last decade [@camacho2020embeddings].

An early way to represent a word for NLP systems was to have a vocabulary of words, and each word is represented as an atomic, simple index into the vocabulary [@mikolov2013word2vec]. Embeddings are an alternative method to encode natural language as a vector representation. Embeddings can be used to represent words [@bengio2000neural]; [@collobert2011natural]; [@pennington2014glove], sentences [@kiros2015skip] and documents [@pilehvar2020embeddings], as well as to encode more specific meanings of natural language for tasks such as sentiment analysis, topic categorization and word sense disambiguation [@pilehvar2020embeddings]. 

The use of word embeddings in NLP is an improvement over representing words as an index into a vocabulary since embeddings can encode relationships or similarity of words. For example, when using a simple index into a vocabulary to represent a word, “boy” would have one index and “man” would have another index, but there is no indication that these two words are related. Vocabulary indexes also fail to represent that a word may have multiple meanings. For example, “mouse” would be one entry in a vocabulary, but would fail to indicate if the word refers to an animal, or computer input device [@camacho2020embeddings].

Word embedding models can capture fairly detailed semantic and syntactic patterns, however how these patterns are encoded in a vector is often unclear [@pennington2014glove]. Unlike indexes into a vocabulary, word embeddings have the advantage of being able to be compared for similarity. Comparison is often done either by measuring distance between vectors, or the angle between vectors [@pennington2014glove]. [@mikolov2013word2vec] discovered in their famous “Word2Vec” model that simple vector arithmetic over their embedding model allowed evaluation of analogies. For example, $vector(king) - vector(man) + vector (woman)$ resulted in a vector that was closest to the vector for “queen”.

Embedding models also exist for more complicated NLP problems than word representations. Sentence embedding models can be built on top of word embeddings models. This is an improvement over using “bag of words” representations for sections of natural language longer than a single word. Bag of word models often use vectors of one-hot encoding, however these representations have very high dimensionality and sparseness. These issues led researchers to look for alternatives and they found embeddings to be a valuable technique [@pilehvar2020embeddings]. Sentence embeddings compose word embeddings for each of the words of a sentence into a single vector that encodes semantic and syntactic properties of the sentence and can be compared for similarity across those properties [@kiros2015skip]. As with using bag of words to encode sentences, bag of word models are similarly poor representations of documents for many NLP tasks. In such cases, encoding the document into a document embedding is a useful alternative representation [@pilehvar2020embeddings]. Since bag of word models ignore word ordering, they are a problematic representation for many NLP tasks, including sentiment analysis [@pilehvar2020embeddings], which will be the focus for the rest of this work.

### 1.3 Using Embeddings For Sentiment Analysis
Now that we have touched on the topics of sentiment analysis and embeddings, let's dive into the use of embeddings for sentiment analysis. The performance of natural language processing (NLP) tasks including question answering & sentiment analysis, has significantly improved due to word embedding and deep learning models over the years [@10255396]. Ultimately our goal when converting text into embeddings is to be able to extract the meaning of the words in the text and allow our model to learn from it. There are two very popular word embedding methods, of which we have already touched upon slightly, the two methods are Word2Vec and Global Vectors (GloVe). Using one of these two methods along with an embedding model we will be able to categorize data into positive, negative, or neutral sentiments by evaluating the overall sentiment of the input data. This branch of NLP has been increasingly becoming more and more popular, specially for processing and gathering the sentiment of posts on social media sites, forums, an the web in general. 

Our two popular pretrained word  methods were each trained on different datasets. Word2Vec was trained on a portion of the Google News dataset with 300-dimensional vectors and GloVe was trained on 6 billion tokens, 400k vocabulary words, and 100-dimensional vectors from Wikipedia 2014 [@10543279]. The drawback to most embedding based models is that they typically only utilize pre-trained word embeddings that can not capture the effective information in the text or input,  such as the contextual sentiment information of both targets and aspects [@9397340]. This is problematic because this leads words with similar contexts but opposite sentiment (good and bad for example) to be mapped to neighboring word vectors making our models less accurate[@7296633]. We will be tackling this issue more in depth in this paper and exploring other forms of embeddings for sentiment analysis and models along the way!


## 2. Methods
In this section we will be implementing a neural model using the keras package, which uses a tensorflow backend to classify the sentiment in tweets. Our first step was to download and set up that environment on our computers. Before starting to work with our data we first needed to pre-process it and ensure that our data was clean and was composed of all lowercase letters and that it had all non-alphabetic characters, stop words, emojis, URLs, and usernames removed. From there we needed to vectorize or convert our strings into numbers which we accomplished by encoding each word with a unique number to create a vocabulary list of all unique words in our tweets dataset. An example follows below:

Tweet #1: "this is fun"

Tweet #2: "fun times ahead"

Tweet #3: "fun is ahead of times"

Based on our three tweets above, we have six unique words. These six unique words would each be assigned a numeric value. Thus our vocabulary list would be [1-this, 2-is, 3-fun, 4-times, 5-ahead, 6-of]. So to vectorize our tweets we would just need to put the numeric values for each of those words together to represent the three tweets above as shown below.

Tweet #1 vectorized: [1,2,3]

Tweet #2 vectorized: [3,4,5]

Tweet #3 vectorized: [3,2,5,6,4]

After creating our vocabulary list from all the unique words in our tweets dataset we had to set a limit to the number of inputs (numeric) we would be accepting for our model. Our neural network has a simple architecture of 500 inputs to the embedding layer and 7 outputs or 7 dimensions of the embedding vectors we create. [@DeepLearningRBook] was an important resource in coding the embedding model. The embedding layer acts as a lookup table that maps the integer indices from our vocabulary list to the dense vectors which are the vectorized tweets. In other words, the maximum number of words we would use from a tweet which we decided to go with 500. If any tweet was less than 500 words we would then just pad that vectorized tweet with zeros to reach the 500 numeric inputs that the model is expecting.

After having accomplished all of the above, our last step was to split our dataset into training and test sets and compile and train our model. We decided to go with a 70% training set and a 30% test set. Our analysis and results section follows below with what we learned in this section.






## 3. Analysis and Results
### 3.1 Dataset Description
For sentiment analysis, we will be using a data set that contains computer generated tweets for training data and another collection of tweets for testing. Any tweet with positive emoticons was assumed to be positive, and any tweet with negative emotions was assumed to be negative. The emoticons were removed from the dataset. The dataset was last updated 4 years ago and was posted by Abhishek Shrivastava. The computer-generated tweets that make up the training data are organized into ten columns. The first column is textID used to uniquely identify each tweet in the dataset. The second column is the text which is the entire tweet used. The third column is the selected text of the tweet used to perform sentiment analysis. The fourth column is the labeled sentiment of the tweet, which is classified as neutral, negative, or positive. The fifth column is the time of day the tweet was posted which is classified as morning, noon, or night. The sixth column stores the age range that the user is in. The seventh column is the country the tweet was posted from when the user posted. The eighth column is the given population of the country. The ninth column is the given land area of the country. The tenth column is the given population density of the country. During the literature review process, twitter datasets were often used for sentiment analysis. Therefore, this dataset of generated tweets seems appropriate for our sentiment analysis research project. [@Khurana2023], [@Sawicki2023], [@9036670], [@Chong2014].

#### Install and Import Packages

```{r}
#install.packages('tidyverse', repos = "http://cran.us.r-project.org")
#install.packages('tm', repos = "http://cran.us.r-project.org")
#install.packages('SnowballC', repos = "http://cran.us.r-project.org")
#install.packages('fastDummies', repos = "http://cran.us.r-project.org")
#install.packages('reticulate', repos = "http://cran.us.r-project.org")
#install.packages('tensorflow', repos = "http://cran.us.r-project.org")
#remove.packages("keras")
#install.packages('keras3', repos = "http://cran.us.r-project.org")
#install.packages('dplyr', repos = "http://cran.us.r-project.org")
#install.packages('text2vec', repos = "http://cran.us.r-project.org")
#install.packages('tidyr', repos = "http://cran.us.r-project.org")
library(tidyverse)
library(tm)
library(SnowballC)
library(fastDummies)
```

#### Loading and preparing Training and Test Data

```{r}

#load data
# drop all sentiment of 1 (neutral), and change all values of 2 to 1
# final result will be sentiment=0: negative, sentiment=1: positive
train <- read.csv('train.csv') %>% filter(sentiment!=1) %>% 
  mutate(sentiment = ifelse(sentiment==2, 1, sentiment))
test <- read.csv('test.csv') %>% filter(sentiment!=1) %>% 
  mutate(sentiment = ifelse(sentiment==2, 1, sentiment))

#convert data to corpus
mycorpus_train <- Corpus(VectorSource(train$text))
mycorpus_test <- Corpus(VectorSource(test$text))

# Create Functions to Clean Data
remove_url <- function(x) {
  gsub("http[^[:space:]]*", "", x)
}


remove_mentions <- function(x) {
  gsub("@\\w+", "", x)
}


remove_hashtags <- function(x) {
  gsub("#\\w+", "", x)
}

remove_rt <- function(x) {
  gsub("\\bRT\\b", "", x)
}


remove_punctuation <- function(x) {
  gsub("[[:punct:]]+", " ", x)  # Replace punctuation with a space
}

convert_to_lowercase <- function(x) {
  tolower(x)
}

remove_numbers <- function(x) {
  gsub("\\d+", "", x)
}

remove_non_ascii <- function(x) {
  iconv(x, "latin1", "ASCII", sub="")
}

remove_stopwords <- function(x) {
  removeWords(x, stopwords("en"))
}

stem_words <- function(x) {
  wordStem(x, language = "en")
}

remove_extra_whitespace <- function(x) {
  gsub("\\s+", " ", trimws(x))
}

# Execute cleaning Functions
mycorpus_train <- tm_map(mycorpus_train, content_transformer(remove_url))
mycorpus_train <- tm_map(mycorpus_train, content_transformer(remove_punctuation))
mycorpus_train <- tm_map(mycorpus_train, content_transformer(convert_to_lowercase))
mycorpus_train <- tm_map(mycorpus_train, content_transformer(remove_stopwords))
mycorpus_train <- tm_map(mycorpus_train, content_transformer(stem_words))
mycorpus_train <- tm_map(mycorpus_train, content_transformer(remove_numbers))
mycorpus_train <- tm_map(mycorpus_train, content_transformer(remove_hashtags))
mycorpus_train <- tm_map(mycorpus_train, content_transformer(remove_rt))
mycorpus_train <- tm_map(mycorpus_train, content_transformer(remove_mentions))
mycorpus_train <- tm_map(mycorpus_train, content_transformer(remove_extra_whitespace))

mycorpus_test <- tm_map(mycorpus_test, content_transformer(remove_url))
mycorpus_test <- tm_map(mycorpus_test, content_transformer(remove_punctuation))
mycorpus_test <- tm_map(mycorpus_test, content_transformer(convert_to_lowercase))
mycorpus_test <- tm_map(mycorpus_test, content_transformer(remove_stopwords))
mycorpus_test <- tm_map(mycorpus_test, content_transformer(stem_words))
mycorpus_test <- tm_map(mycorpus_test, content_transformer(remove_numbers))
mycorpus_test <- tm_map(mycorpus_test, content_transformer(remove_hashtags))
mycorpus_test <- tm_map(mycorpus_test, content_transformer(remove_rt))
mycorpus_test <- tm_map(mycorpus_test, content_transformer(remove_mentions))
mycorpus_test <- tm_map(mycorpus_test, content_transformer(remove_extra_whitespace))

# Convert Each Corpus Back to a Data Frame
tweets_train <- data.frame(text = sapply(mycorpus_train,as.character),stringsAsFactors = FALSE)
tweets_test <- data.frame(text = sapply(mycorpus_test,as.character),stringsAsFactors = FALSE)

# One-hot Encode Sentiment Values and Attach New Data Frames to the Text Column
train_sentiment_int = ifelse(train$sentiment=='positive', 0, ifelse(train$sentiment=='neutral',1,2))
test_sentiment_int = ifelse(test$sentiment=='positive', 0, ifelse(test$sentiment=='neutral',1,2))
train_processed <- cbind(tweets_train, train_sentiment_int)
test_processed <- cbind(tweets_test, test_sentiment_int)
names(train_processed) = c('text','sentiment')
names(test_processed) = c('text','sentiment')

# Drop NA Values
train_processed <- train_processed %>% filter(!is.na(text) & text != "")
test_processed <- test_processed %>% filter(!is.na(text) & text != "")
```

### Data and Visualization


### Statistical Modeling

```{r}
#| output: false
#| warning: false
library(text2vec)
library(dplyr)
library(tidyr)

reticulate::install_python()
library(keras3)
install_keras(envname="r-tensorflow")
```

```{r}
max_vocab_words <- 5000
maxlen <- 500
vector_dimensions <- 6

# First create a vocabulary of all the individual words used in the training dataset
it = itoken(train_processed$text, preprocessor = tolower, tokenizer=word_tokenizer)
vocab <- create_vocabulary(it) 
vocab_sorted <- vocab[order(vocab$term_count, decreasing=TRUE),] %>% mutate(idx = row_number())
vocab_sorted <- head(vocab_sorted, n=max_vocab_words-1) # take only the top most common words

# Now create a mapping from the vocabulary word to a integer
# representation for each word. Use factor indexing so the word
# can be looked up quickly and retrieve the mapped integer value.
vocab_translate = vocab_sorted$idx
names(vocab_translate) = as.factor(vocab_sorted$term)

```

```{r}

# Vectorize every tweet in the dataset by looking up the integer index
# from the vocabulary associated with each word in the tweet.
vectorize_row <- function(r) {
  tokenized = word_tokenizer(r)[[1]]
  vectorized <- c(sapply(tokenized, function(x) vocab_translate[x]), rep(0,maxlen-length(tokenized)))
  return(vectorized)
}

tokenized_train <- sapply(train_processed$text, vectorize_row)
tokenized_train <- t(tokenized_train) # transpose since sapply returns the tweets in columns instead of rows
tokenized_train[is.na(tokenized_train)] <- 0 # any word outside our top vocab is NA, so replace with 0 for unknown word
dimnames(tokenized_train) <- list(NULL, NULL) # remove the tweets as indexes into the matrix

train_y <- train_processed %>% select(sentiment)
train_x <- train_processed %>% select(-sentiment)
test_y <- test_processed %>% select(sentiment)
test_x <- test_processed %>% select(-sentiment)

tokenized_test <- sapply(test_x$text, vectorize_row)
tokenized_test <- t(tokenized_test) # transpose since sapply returns the tweets in columns instead of rows
tokenized_test[is.na(tokenized_test)] <- 0 # any word outside our top vocab is NA, so replace with 0 for unknown word
dimnames(tokenized_test) <- list(NULL, NULL) # remove the tweets as indexes into the matrix

tokenized_train <- sapply(train_x$text, vectorize_row)
tokenized_train <- t(tokenized_train) # transpose since sapply returns the tweets in columns instead of rows
tokenized_train[is.na(tokenized_train)] <- 0 # any word outside our top vocab is NA, so replace with 0 for unknown word
dimnames(tokenized_train) <- list(NULL, NULL) # remove the tweets as indexes into the matrix
```

```{r}
embedding_model <- keras_model_sequential() %>%
  layer_embedding(input_dim = max_vocab_words, output_dim = vector_dimensions) %>%
  layer_flatten() %>%
  layer_dense(units = 1, activation = "sigmoid")
embedding_model %>% keras3::compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = c("acc")
)
summary(embedding_model)
```

```{r}
embedding_history <- embedding_model %>% keras3::fit(
  tokenized_train, train_y, 
  epochs = 6,
  batch_size = 32,
  validation_split = 0.2
)
```

```{r}
embedding_results <- embedding_model %>% evaluate(tokenized_test, test_y)
```

### Conclusion

